{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "*Part of the course:\n",
    "Machine Learning (code: INFOB3ML), fall 2022, Utrecht University*\n",
    "\n",
    "Total points: 10\n",
    "\n",
    "Deadline: Friday 16 September, 23:59\n",
    "\n",
    "**Write your names and student numbers here: ___**\n",
    "\n",
    "Submit one ipynb file per pair.\n",
    "\n",
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Regularisation\n",
    "In this assignment, you will perform several simulation experiments with linear regression, in order to investigate the effects of regularisation.\n",
    "\n",
    "Those who followed the course Introduction to Machine Learning will see that this assignment resembles the linear regression assignment from that course. If you didn't take that course, don't worry: everything you need to know about it is also in chapter 1 of the Rogers & Girolami book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Data generation\n",
    "Every datapoint $(x,t)$ will be sampled randomly, with both $x$ and $t$ in $\\mathbb{R}$. (Note that the book from Introduction to Machine Learning called the output $y$, while this course's book calls it $t$.) The input $x$ is uniformly distributed between $-1$ and $1$.\n",
    "\n",
    "Once we have our input $x$, we generate our output $t$ according to $t = 1 - \\cos(x) + \\epsilon$, where $\\epsilon$ is normally distributed with expected value = 0 and variance = $\\sigma^2$.\n",
    "All the random numbers should be generated **independently** from one another.\n",
    "\n",
    "### Regression\n",
    "\n",
    "We'll implement *regularised* regression, adding a penalty $\\lambda \\mathbf{w}^T \\mathbf{w}$ to our training loss. Linear regression with this form of regularisation penalty is also called *ridge regression*. We are going to try out different values of $\\lambda$.\n",
    "\n",
    "We'll use regression with order five polynomials like in the book. This means that for each weight vector $\\mathbf{w}$, our hypothesis is of the form\n",
    "$$f(x; \\mathbf{w}) = \\sum_{i=0}^5 w_i x^i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Code\n",
    "\n",
    "To make it clear what your code is supposed to do and how it should be formatted, we provide you general schema for each to-be-written function. Some functions come with additional hints about useful in-built functions or procedural details. You might write the function's body differently than the hints suggests. That's totally fine as long as the function works as it supposed to work.\n",
    "\n",
    "Use numpy for functionalities involving vectors and matrices. [Here is a handy guide (in notebook form) that deals with numpy arrays, matrices and number generation.](https://github.com/ageron/handson-ml/blob/master/tools_numpy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1** (1 point)\n",
    "\n",
    "Write a function `generate_data` you can use to generate a dataset and outputs the pair of vectors `(x,t)`, accepting parameters $N$ and $\\sigma^2$. Be sure to check if your normal-distribution-generator needs $\\sigma$ (standard deviation) or $\\sigma^2$ (variance) as input parameter. Both `x` and `t` should be 1-dimensional numpy arrays, i.e. their shape should be `(N,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# Some functions you may find useful (here and later):\n",
    "# np.cos, np.ones, np.random.random_sample, np.random.randn,\n",
    "# np.linalg.inv, np.hstack, np.matmul (or @), np.dot, math.sqrt.\n",
    "\n",
    "def generate_data(N, sigma_squared):\n",
    "    x = np.random.uniform(-1,1,N)\n",
    "    t = np.zeros(N)\n",
    "    for i in range(len(x)):\n",
    "        t[i] = 1 - np.cos(x[i]) + math.sqrt(sigma_squared)\n",
    "    return x,t\n",
    "            \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[0.09762701 0.43037873 0.20552675]\n",
      "(3,)\n",
      "[0.3209895  0.40741996 0.33727415]\n"
     ]
    }
   ],
   "source": [
    "# ██████████ TEST ██████████\n",
    "# (These \"TEST\" blocks can help you quickly check if there's something obviously wrong with the code you wrote.)\n",
    "# Setting a seed helps to make the data generation deterministic for comparison reasons\n",
    "np.random.seed(0) \n",
    "toy_xs, toy_t = generate_data(3, 0.1)\n",
    "# Check if the shapes of the output arrays are as specified above:\n",
    "print(toy_xs.shape)\n",
    "print(toy_xs)\n",
    "print(toy_t.shape)\n",
    "print(toy_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to learn 5th-order polynomials, we want to have a function that computes the matrix $\\mathbf{X}$. (See equation (1.18) on page 28 of the book for what this matrix should look like.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** (1 point)\n",
    "\n",
    "(a) When dealing with $N$ data points, what should the size of the $\\mathbf{X}$-matrix be? (Give the number or rows and the number of columns.)\n",
    "\n",
    "(b) Where in the matrix will you find the vector `x_scalars`?\n",
    "\n",
    "(c) Where in the matrix will you find $\\mathbf{x}_1$ (the feature vector associated to the first data point)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) $N$ number of rows and and 6 columns.**\n",
    "\n",
    "**(b) In the columns 2-6.**\n",
    "\n",
    "**(c) In the second column of the matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 3** (1 point)\n",
    "\n",
    "Write a function `compute_X_matrix` that takes a numpy array `x_scalars` as produced by your code above, and returns the matrix `X` needed by linear regression with 5th-order polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_X_matrix(x_scalars):\n",
    "    X = np.ones((len(x_scalars),6))\n",
    "    for i in range(len(X)):\n",
    "        for j in range(6):\n",
    "            X[i][j] = x_scalars[i]**j\n",
    "    return(X)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09762701 0.43037873 0.20552675]\n",
      "[[1.00000000e+00 9.76270079e-02 9.53103266e-03 9.30486201e-04\n",
      "  9.08405836e-05 8.86849437e-06]\n",
      " [1.00000000e+00 4.30378733e-01 1.85225854e-01 7.97172681e-02\n",
      "  3.43086168e-02 1.47656990e-02]\n",
      " [1.00000000e+00 2.05526752e-01 4.22412458e-02 8.68170607e-03\n",
      "  1.78432285e-03 3.66726080e-04]]\n"
     ]
    }
   ],
   "source": [
    "# ██████████ TEST ██████████\n",
    "toy_X = compute_X_matrix(toy_xs)\n",
    "print(toy_xs)\n",
    "print(toy_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 4** (1 point)\n",
    "\n",
    "Write code that fits a regularised linear regression hypothesis to training data, in other words, a function that computes our $\\hat{\\mathbf{w}}$. Use numpy to carry out the necessary matrix operations to find an analytic solution; don't use linear regression functionality from Python packages for machine learning. In other words, compute $\\hat{\\mathbf{w}}$ according to the equation (1.21) on page 36 of the book. Give your function a parameter `lamb` which tells it the value of $\\lambda$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, t, lamb):\n",
    "    N = len(X)\n",
    "    \n",
    "    XT_X = np.dot(X.T, X)\n",
    "    N_lamb_I = XT_X + np.dot((N * lamb), np.identity(6)) \n",
    "    XT_t = np.dot(X.T, t)\n",
    "    w = np.dot(np.linalg.inv(N_lamb_I), XT_t)\n",
    "    \n",
    "    return w\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 9.76270079e-02 9.53103266e-03 9.30486201e-04\n",
      "  9.08405836e-05 8.86849437e-06]\n",
      " [1.00000000e+00 4.30378733e-01 1.85225854e-01 7.97172681e-02\n",
      "  3.43086168e-02 1.47656990e-02]\n",
      " [1.00000000e+00 2.05526752e-01 4.22412458e-02 8.68170607e-03\n",
      "  1.78432285e-03 3.66726080e-04]]\n",
      "[0.3209895  0.40741996 0.33727415]\n",
      "[0.30405459 0.16621466 0.07871011 0.03534082 0.0155103  0.00673695]\n",
      "[3.51462222e-03 9.10582080e-04 3.06144746e-04 1.17842468e-04\n",
      " 4.82188680e-05 2.02743083e-05]\n"
     ]
    }
   ],
   "source": [
    "# ██████████ TEST ██████████\n",
    "# this test uses values for toy_X and toy_t printed above\n",
    "# You migh need to define these values by hand\n",
    "# if your \"generate_data\" returns different data\n",
    "print(toy_X)\n",
    "print(toy_t)\n",
    "print(fit_ridge(toy_X, toy_t, 0.01))\n",
    "print(fit_ridge(toy_X, toy_t, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared loss over the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code (which you don't need to change) evaluates a learned linear regression function $\\hat{\\mathbf{w}}$ with respect to the data $\\mathbf{X}, \\mathbf{t}$ using the squared error loss. This Python function can be used to compute training, validation or test loss for $\\hat{\\mathbf{w}}$, depending of the kind of data passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(w, X, t):\n",
    "    N, k = X.shape\n",
    "    t_hat = X @ w\n",
    "    t_error = t_hat - t\n",
    "    sum_of_squared_errors = t_error.T @ t_error\n",
    "    loss = sum_of_squared_errors / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to do some experiments and look at the results.\n",
    "\n",
    "First, the function provided below will plot the target function $1 - \\cos(x)$, a regression function, and the training data all in one plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_result(xs_train, t_train, w, include_target_function=True):\n",
    "    xs_plot = np.linspace(-2, 2, 101)\n",
    "    X_plot = compute_X_matrix(xs_plot)\n",
    "    if include_target_function:\n",
    "        plt.plot(xs_plot, 1 - np.cos(xs_plot), c='black') # target \n",
    "    plt.scatter(xs_train, t_train, c='blue', marker=\".\")\n",
    "    plt.plot(xs_plot, X_plot @ w, c='red')\n",
    "    plt.ylim(-1, 1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 5** (1 point)\n",
    "\n",
    "Generate a dataset of $N=15$ data points with noise level $\\sigma^2 = 0.1$. (You'll use this dataset in assignments/questions 5 through 7.) For the values of $\\lambda$ provided in the code below, fit a regularised regression curve to the data and compute the loss. Display the results in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambdas = 10 ** np.linspace(-5, 5, 11)\n",
    "losses = np.zeros_like(lambdas)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_scalars, t = generate_data(15, 0.1)\n",
    "X = compute_X_matrix(x_scalars)\n",
    "for i in range(len(lambdas)):\n",
    "    w = fit_ridge(X, t, lambdas[i])\n",
    "    losses[i] = compute_loss(w, X, t)\n",
    "\n",
    "plt.semilogx(lambdas, losses, c='black') # target \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6** (1 point) Your plot should show that as $\\lambda$ gets larger, the loss also gets larger. Explain why this is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment/Question 7** (1 point)\n",
    "\n",
    "Using the same data, answer the following questions:\n",
    "\n",
    "For what values of $\\lambda > 0$ do you clearly see overfitting? For what values of $\\lambda$ do you see underfitting? To support your answer, include plots for some values of $\\lambda$, and point out what features of those plots tell you that over-/underfitting is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good value of $\\lambda$, a variety of techniques exist. One that obviously does *not* work is to look at the training loss as a function of $\\lambda$ (like you plotted above): that would always suggest to make $\\lambda$ as small as possible! A versatile technique that you've already seen in an earlier course (or in section 1.5 of the book) is **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 8** (1 point)\n",
    "\n",
    "Write some code to do the following: sample a new dataset of $N = 50$ data points and $\\sigma^2=0.1$. (You'll use this dataset for all the remaining assignments and questions.) Write a function that, given data and value of $\\lambda$, computes the leave-one-out cross-validation (LOOCV) loss, as explained in section 1.5.2 of the book. Then make a plot similar to what we did in assignment 5 for the training loss, but this time displaying the LOOCV loss as a function of $\\lambda$.\n",
    "\n",
    "Note that the third argument, `fitting_function`, should be the name of a function that `LOOCV` can call to compute `w`. If `fit_ridge` is passed, your previously written function will be used. But later, you'll call it with a different fitting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.34783226639948e-09\n",
      "9.185504057766287e-07\n",
      "3.709935106903328e-05\n",
      "0.00027753411755214214\n",
      "0.0029728569073478144\n",
      "0.055991738305168716\n",
      "0.2032426701419561\n",
      "0.24890492150751575\n",
      "0.25433658273430115\n",
      "0.2548898756265759\n",
      "0.25494530789081593\n"
     ]
    }
   ],
   "source": [
    "def LOOCV(X, t, fitting_function, lamb):\n",
    "    N, k = X.shape\n",
    "    sum_of_losses = 0.0\n",
    "    for leave_out in range(N):\n",
    "        new_X = np.delete(X, leave_out, 0)\n",
    "        new_t = np.delete(t, leave_out, 0)\n",
    "        \n",
    "        w = fitting_function(new_X, new_t, lamb)\n",
    "        \n",
    "        sum_of_losses += compute_loss(w, X[leave_out : leave_out + 1], t[leave_out : leave_out + 1])\n",
    "        \n",
    "    return sum_of_losses / N\n",
    "\n",
    "x_scalars, t = generate_data(50, 0.1)\n",
    "X = compute_X_matrix(x_scalars)\n",
    "for lamb in lambdas:\n",
    "    print(LOOCV(X, t, fit_ridge, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9** (0.5 points): What value of $\\lambda$ does LOOCV point you to? Look at a plot of the resulting regression function. Does it look reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the book mentions, when doing regularisation, using the squares of $\\mathbf{w}$ as a penalty is just one of many possibilities. It has the advantage of having an analytical solution. But other options exist that may have other advantages, and while they may not be analytically computable, still there exist efficient algorithms for working with them. A particularly popular one is to use the sum of absolute values of $\\mathbf{w}$ as a penalty: we will find the $\\mathbf{w}$ that minimizes\n",
    "$$\\mathcal{L}' = \\mathcal{L} + \\lambda \\sum_i \\lvert w_i \\rvert.$$\n",
    "This is called the 'lasso' (which is an acronym for 'least absolute shrinkage and selection operator', but of course most people just remember the acronym)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no direct formula for computing the $\\mathbf{w}$ that minimizes $\\mathcal{L}'$. The next alternative would be to use (stochastic) gradient descent. Unfortunately, that also doesn't work very nicely here, because as a function of $\\mathbf{w}$, $\\mathcal{L}'$ is not differentiable wherever $\\mathbf{w}$ has at least one entry equal to zero. But variants of gradient descent have been developed that can deal with this problem (such as [proximal gradient descent](https://en.wikipedia.org/wiki/Proximal_gradient_method)), and implementations are readily available. The fitting function provided below uses such an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def fit_lasso(X, t, lamb):\n",
    "    clf = Lasso(lamb, fit_intercept=False, max_iter=100000)\n",
    "    clf.fit(X, t)\n",
    "    return clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 10** (0.5 point) Again plot the LOOCV losses as a function of $\\lambda$, but this time for lasso regression instead of ridge regression. Read off the values of $\\lambda$ that minimizes the LOOCV loss, and display the regression function for that $\\lambda$ in a separate plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of lasso regularisation is its tendency to make some weigths exactly equal to 0. (Well, mathematically that's true, but you should never rely on things being *exactly* equal when a numerical algorithm is involved. Instead, check whether the difference between them is very small, say less than `1e-9`.)\n",
    "\n",
    "**Assignment 11** (0.5 points)\n",
    "\n",
    "What is the smallest $\\lambda$ in `lambdas` for which you observe this happening for some $w_i$? For that $\\lambda$, make a plot where $w_i$ varies along the horizontal axis. On the vertical axis, plot the regularised loss $\\mathcal{L}'$ of the weight vector, with all entries other than $w_i$ kept equal to the optimal lasso solution. Choose the range of $w_i$-values small enough that you see a nondifferentiability in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12** (0.5 points): Use this graph to explain why lasso regression has a tendency to make some weights equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember: Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
